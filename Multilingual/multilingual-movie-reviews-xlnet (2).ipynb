{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4921795,"sourceType":"datasetVersion","datasetId":2854302}],"dockerImageVersionId":30588,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-05T02:11:48.932812Z","iopub.execute_input":"2023-12-05T02:11:48.933517Z","iopub.status.idle":"2023-12-05T02:11:48.941982Z","shell.execute_reply.started":"2023-12-05T02:11:48.933482Z","shell.execute_reply":"2023-12-05T02:11:48.940992Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"/kaggle/input/movie-reviews-dataset/test.tsv\n/kaggle/input/movie-reviews-dataset/train.tsv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install --upgrade numpy\n!pip install --upgrade SciPy","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:11:13.252129Z","iopub.execute_input":"2023-12-05T02:11:13.252719Z","iopub.status.idle":"2023-12-05T02:11:39.645572Z","shell.execute_reply.started":"2023-12-05T02:11:13.252683Z","shell.execute_reply":"2023-12-05T02:11:39.644541Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (1.26.1)\nCollecting numpy\n  Obtaining dependency information for numpy from https://files.pythonhosted.org/packages/64/41/284783f1014685201e447ea976e85fed0e351f5debbaf3ee6d7645521f1d/numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Using cached numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\nUsing cached numpy-1.26.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\nInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.26.1\n    Uninstalling numpy-1.26.1:\n      Successfully uninstalled numpy-1.26.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ncudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ncuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\ndask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\napache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\napache-beam 2.46.0 requires numpy<1.25.0,>=1.14.3, but you have numpy 1.26.2 which is incompatible.\napache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 11.0.0 which is incompatible.\ncudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ncudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\ncuml 23.8.0 requires dask==2023.7.1, but you have dask 2023.11.0 which is incompatible.\ncuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.11.0 which is incompatible.\ndask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2023.11.0 which is incompatible.\ndask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.11.0 which is incompatible.\ndask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\ndask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2023.11.0 which is incompatible.\ndask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.11.0 which is incompatible.\ndask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.0.3 which is incompatible.\nlibpysal 4.9.2 requires packaging>=22, but you have packaging 21.3 which is incompatible.\nlibpysal 4.9.2 requires shapely>=2.0.1, but you have shapely 1.8.5.post1 which is incompatible.\nmomepy 0.6.0 requires shapely>=2, but you have shapely 1.8.5.post1 which is incompatible.\nnumba 0.57.1 requires numpy<1.25,>=1.21, but you have numpy 1.26.2 which is incompatible.\npymc3 3.11.5 requires numpy<1.22.2,>=1.15.0, but you have numpy 1.26.2 which is incompatible.\npymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.11.4 which is incompatible.\nraft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2023.11.0 which is incompatible.\nraft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2023.11.0 which is incompatible.\ntensorflow 2.13.0 requires numpy<=1.24.3,>=1.22, but you have numpy 1.26.2 which is incompatible.\ntensorflowjs 4.13.0 requires packaging~=23.1, but you have packaging 21.3 which is incompatible.\nwoodwork 0.26.0 requires numpy<1.25.0,>=1.22.0, but you have numpy 1.26.2 which is incompatible.\nydata-profiling 4.5.1 requires numpy<1.24,>=1.16.0, but you have numpy 1.26.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.26.2\nRequirement already satisfied: SciPy in /opt/conda/lib/python3.10/site-packages (1.11.4)\nRequirement already satisfied: numpy<1.28.0,>=1.21.6 in /opt/conda/lib/python3.10/site-packages (from SciPy) (1.26.2)\n","output_type":"stream"}]},{"cell_type":"code","source":"# Importing stock ml libraries\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nimport transformers\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom transformers import XLNetModel, XLNetTokenizer\n\n# Preparing for TPU usage\n# import torch_xla\n# import torch_xla.core.xla_model as xm\n# device = xm.xla_device()\n\n# # Setting up the device for GPU usage\nfrom torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:11:54.726837Z","iopub.execute_input":"2023-12-05T02:11:54.727498Z","iopub.status.idle":"2023-12-05T02:11:54.733327Z","shell.execute_reply.started":"2023-12-05T02:11:54.727464Z","shell.execute_reply":"2023-12-05T02:11:54.732353Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#load data\ntrain_df = pd.read_csv(\"/kaggle/input/movie-reviews-dataset/train.tsv\", sep='\\t')\ntest_df = pd.read_csv(\"/kaggle/input/movie-reviews-dataset/test.tsv\", sep='\\t')\n\n# # merge the tables by Body ID\n# train_df = pd.merge(df_body_train, df_stance_train, on='Body ID', how='inner')\n# test_df = pd.merge(df_body_test, df_stance_test, on='Body ID', how='inner')\n\n# null_counts_train = train_df.isnull().sum() #no nulls\n# null_counts_test = test_df.isnull().sum()  #no nulls\n\ntotal_rows_train = len(train_df)\ntotal_rows_test = len(test_df)\n\nunique_label_train = train_df['label'].unique()\nunique_label_test = test_df['label'].unique()\n\nprint(\"TRAIN: Total number of rows: \",total_rows_train,\", Unique labels:\",unique_label_train)\nprint(\"TEST: Total number of rows: \",total_rows_test,\", Unique labels:\",unique_label_test)\n\n# removing \"n//a\" rows and reprinting\n\ntrain_df = train_df[train_df['label'] != 'n\\\\a']\n\ntotal_rows_train = len(train_df)\ntotal_rows_test = len(test_df)\n\nunique_label_train = train_df['label'].unique()\nunique_label_test = test_df['label'].unique()\n\nprint(\"TRAIN: Total number of rows: \",total_rows_train,\", Unique labels:\",unique_label_train)\nprint(\"TEST: Total number of rows: \",total_rows_test,\", Unique labels:\",unique_label_test)\n\n# print(train_df.head())\n# print(test_df.head())\n\n# convert the last column i.e. the categorical column to a one hot encoded list. \ntrain_df['label'] = pd.get_dummies(train_df['label'],columns=train_df.columns).astype(int).values.tolist()\nnew_df_train = train_df[['lang','review', 'label']].copy()\n# Passing colums as train.columns so that the encoding is consistent among train and test\ntest_df['label'] = pd.get_dummies(test_df['label'],columns=train_df.columns).astype(int).values.tolist()\nnew_df_test = test_df[['lang','review', 'label']].copy()\n\n# train_df['label'] = train_df['label'].apply(lambda x: 1 if x == 'pos' else 0 if x == 'neg' else None)\n# new_df_train = train_df[['lang','review', 'label']].copy()\n\n# test_df['label'] = test_df['label'].apply(lambda x: 1 if x == 'pos' else 0 if x == 'neg' else None)\n# new_df_test = test_df[['lang','review', 'label']].copy()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:11:57.831767Z","iopub.execute_input":"2023-12-05T02:11:57.832548Z","iopub.status.idle":"2023-12-05T02:12:09.315020Z","shell.execute_reply.started":"2023-12-05T02:11:57.832510Z","shell.execute_reply":"2023-12-05T02:12:09.314078Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"TRAIN: Total number of rows:  79852 , Unique labels: ['pos' 'n\\\\a' 'neg']\nTEST: Total number of rows:  10000 , Unique labels: ['neg' 'pos']\nTRAIN: Total number of rows:  43145 , Unique labels: ['pos' 'neg']\nTEST: Total number of rows:  10000 , Unique labels: ['neg' 'pos']\n","output_type":"stream"}]},{"cell_type":"code","source":"new_df_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:12:59.523899Z","iopub.execute_input":"2023-12-05T02:12:59.524269Z","iopub.status.idle":"2023-12-05T02:12:59.542761Z","shell.execute_reply.started":"2023-12-05T02:12:59.524241Z","shell.execute_reply":"2023-12-05T02:12:59.541841Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"  lang                                             review   label\n0   de  Einen Ausweg gibt es vielleicht für Daru (Vigg...  [0, 1]\n2   pl  Bardzo lubię kino niemieckie. Jest fenomenalne...  [0, 1]\n3   pl  Całkiem niedawno trafiłem na nie tak świeżą pr...  [0, 1]\n4   cs  V roce 1963, tedy v době, kdy se začalo mimo j...  [0, 1]\n5   pl  Dzieciaki - mogłoby się wydawać, że nie mają n...  [0, 1]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lang</th>\n      <th>review</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>de</td>\n      <td>Einen Ausweg gibt es vielleicht für Daru (Vigg...</td>\n      <td>[0, 1]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>pl</td>\n      <td>Bardzo lubię kino niemieckie. Jest fenomenalne...</td>\n      <td>[0, 1]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>pl</td>\n      <td>Całkiem niedawno trafiłem na nie tak świeżą pr...</td>\n      <td>[0, 1]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>cs</td>\n      <td>V roce 1963, tedy v době, kdy se začalo mimo j...</td>\n      <td>[0, 1]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>pl</td>\n      <td>Dzieciaki - mogłoby się wydawać, że nie mają n...</td>\n      <td>[0, 1]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"new_df_test.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:13:05.892455Z","iopub.execute_input":"2023-12-05T02:13:05.892824Z","iopub.status.idle":"2023-12-05T02:13:05.904254Z","shell.execute_reply.started":"2023-12-05T02:13:05.892795Z","shell.execute_reply":"2023-12-05T02:13:05.903381Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"  lang                                             review   label\n0   pl  Bardzo intensywnie i długo zastanawiałem się, ...  [1, 0]\n1   sk  Režisér Richard Kelly, tvorca skvelého Donnieh...  [0, 1]\n2   sk  Stáva sa to pomerne často. Príprava filmu sa n...  [0, 1]\n3   sk  Tretí diel rebootovaného Star Treku od J. J. A...  [1, 0]\n4   de  „Spieglein, Spieglein an der Wand…“ heißt es i...  [1, 0]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>lang</th>\n      <th>review</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>pl</td>\n      <td>Bardzo intensywnie i długo zastanawiałem się, ...</td>\n      <td>[1, 0]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>sk</td>\n      <td>Režisér Richard Kelly, tvorca skvelého Donnieh...</td>\n      <td>[0, 1]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>sk</td>\n      <td>Stáva sa to pomerne často. Príprava filmu sa n...</td>\n      <td>[0, 1]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>sk</td>\n      <td>Tretí diel rebootovaného Star Treku od J. J. A...</td>\n      <td>[1, 0]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>de</td>\n      <td>„Spieglein, Spieglein an der Wand…“ heißt es i...</td>\n      <td>[1, 0]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"new_df_train['review'].apply(lambda x: len(str(x).split())).max()","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:13:09.629468Z","iopub.execute_input":"2023-12-05T02:13:09.630318Z","iopub.status.idle":"2023-12-05T02:13:11.695663Z","shell.execute_reply.started":"2023-12-05T02:13:09.630284Z","shell.execute_reply":"2023-12-05T02:13:11.694682Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"5703"},"metadata":{}}]},{"cell_type":"code","source":"# Sections of config\n\n# Defining some key variables that will be used later on in the training\nMAX_LEN = 256\nTRAIN_BATCH_SIZE = 8\nVALID_BATCH_SIZE = 4\nEPOCHS = 3\nLEARNING_RATE = 1e-05\ntokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:16:42.577148Z","iopub.execute_input":"2023-12-05T02:16:42.577544Z","iopub.status.idle":"2023-12-05T02:16:42.927130Z","shell.execute_reply.started":"2023-12-05T02:16:42.577513Z","shell.execute_reply":"2023-12-05T02:16:42.926309Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class CustomDataset(Dataset):\n\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n#         self.lang = self.data[\"lang\"]\n        self.review = self.data[\"review\"]\n        self.targets = self.data[\"label\"]\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.review)\n\n    def __getitem__(self, index):\n        review = str(self.review[index])\n        review = \" \".join(review.split())\n#         lang = str(self.lang[index])\n\n        inputs = self.tokenizer(\n            review, \n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=True,\n            truncation='only_first', \n            return_overflowing_tokens=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n        }","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:13:27.046873Z","iopub.execute_input":"2023-12-05T02:13:27.047634Z","iopub.status.idle":"2023-12-05T02:13:27.056617Z","shell.execute_reply.started":"2023-12-05T02:13:27.047600Z","shell.execute_reply":"2023-12-05T02:13:27.055669Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Creating the dataset and dataloader for the neural network\n\ntrain_dataset=new_df_train.sample(frac=1,random_state=200).reset_index(drop=True)\ntest_dataset=new_df_test.sample(frac=1,random_state=200).reset_index(drop=True)\n\n\n# print(\"FULL Dataset: {}\".format(new_df.shape))\nprint(\"TRAIN Dataset: {}\".format(train_dataset.shape))\nprint(\"TEST Dataset: {}\".format(test_dataset.shape))\n\ntraining_set = CustomDataset(train_dataset, tokenizer, MAX_LEN)\ntesting_set = CustomDataset(test_dataset, tokenizer, MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:15:08.140942Z","iopub.execute_input":"2023-12-05T02:15:08.141888Z","iopub.status.idle":"2023-12-05T02:15:08.164656Z","shell.execute_reply.started":"2023-12-05T02:15:08.141844Z","shell.execute_reply":"2023-12-05T02:15:08.163655Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"TRAIN Dataset: (43145, 3)\nTEST Dataset: (10000, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"train_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntest_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 0\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\ntesting_loader = DataLoader(testing_set, **test_params)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:15:56.023621Z","iopub.execute_input":"2023-12-05T02:15:56.024384Z","iopub.status.idle":"2023-12-05T02:15:56.029898Z","shell.execute_reply.started":"2023-12-05T02:15:56.024332Z","shell.execute_reply":"2023-12-05T02:15:56.028853Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"class XLNetClass(torch.nn.Module):\n    def __init__(self):\n        super(XLNetClass, self).__init__()\n        self.l1 = XLNetModel.from_pretrained(\"xlnet-base-cased\")\n        self.pre_classifier = torch.nn.Linear(768, 768)\n        self.dropout = torch.nn.Dropout(0.3)\n        self.classifier = torch.nn.Linear(768, 2)\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n        hidden_state = output_1[0]\n        pooler = hidden_state[:, 0]\n        pooler = self.pre_classifier(pooler)\n        pooler = torch.nn.ReLU()(pooler)\n        pooler = self.dropout(pooler)\n        output = self.classifier(pooler)\n        return output\n    \nmodel = XLNetClass()\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:16:00.865910Z","iopub.execute_input":"2023-12-05T02:16:00.866253Z","iopub.status.idle":"2023-12-05T02:16:08.057715Z","shell.execute_reply.started":"2023-12-05T02:16:00.866227Z","shell.execute_reply":"2023-12-05T02:16:08.056876Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/467M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"caaab73bd654460c92c608595b9d3643"}},"metadata":{}},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"XLNetClass(\n  (l1): XLNetModel(\n    (word_embedding): Embedding(32000, 768)\n    (layer): ModuleList(\n      (0-11): 12 x XLNetLayer(\n        (rel_attn): XLNetRelativeAttention(\n          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ff): XLNetFeedForward(\n          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n          (dropout): Dropout(p=0.1, inplace=False)\n          (activation_function): GELUActivation()\n        )\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n    )\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (pre_classifier): Linear(in_features=768, out_features=768, bias=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (classifier): Linear(in_features=768, out_features=2, bias=True)\n)"},"metadata":{}}]},{"cell_type":"code","source":"def loss_fn(outputs, targets):\n    return torch.nn.BCEWithLogitsLoss()(outputs, targets)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:16:13.225178Z","iopub.execute_input":"2023-12-05T02:16:13.225862Z","iopub.status.idle":"2023-12-05T02:16:13.230091Z","shell.execute_reply.started":"2023-12-05T02:16:13.225824Z","shell.execute_reply":"2023-12-05T02:16:13.229229Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:16:19.186703Z","iopub.execute_input":"2023-12-05T02:16:19.187560Z","iopub.status.idle":"2023-12-05T02:16:19.192630Z","shell.execute_reply.started":"2023-12-05T02:16:19.187525Z","shell.execute_reply":"2023-12-05T02:16:19.191748Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"loss_tracker = []\ndef train(epoch):\n    model.train()\n    for _,data in enumerate(training_loader, 0):\n        ids = data['ids'].to(device, dtype = torch.long)\n        mask = data['mask'].to(device, dtype = torch.long)\n        token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n        targets = data['targets'].to(device, dtype = torch.float)\n\n        outputs = model(ids, mask, token_type_ids)\n\n        optimizer.zero_grad()\n        loss = loss_fn(outputs, targets)\n        if _%1000==0:\n            loss_tracker.append(loss)\n            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        \ndef validation(epoch):\n    model.eval()\n    fin_targets=[]\n    fin_outputs=[]\n    with torch.no_grad():\n        for _, data in enumerate(testing_loader, 0):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n            targets = data['targets'].to(device, dtype = torch.float)\n            outputs = model(ids, mask, token_type_ids)\n            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n    return fin_outputs, fin_targets","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:16:53.046464Z","iopub.execute_input":"2023-12-05T02:16:53.047304Z","iopub.status.idle":"2023-12-05T02:16:53.057951Z","shell.execute_reply.started":"2023-12-05T02:16:53.047270Z","shell.execute_reply":"2023-12-05T02:16:53.057093Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"'''accuracy_tracker = []\nf1_micro_tracker = []\nf1_macro_tracker = []\nfor epoch in range(EPOCHS):\n    print(\"Epoch \",epoch)\n    train(epoch)\n    outputs, targets = validation(epoch)\n    outputs = np.array(outputs) >= 0.5\n    accuracy = metrics.accuracy_score(targets, outputs)\n    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n    accuracy_tracker.append(accuracy)\n    f1_micro_tracker.append(f1_score_micro)\n    f1_macro_tracker.append(f1_score_macro)\n    print(f\"Accuracy Score = {accuracy}\")\n    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n    print(f\"F1 Score (Macro) = {f1_score_macro}\")'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''np.savetxt('multilingual_review_xlnet_loss_tracker.txt',loss_tracker,delimiter=',')\nnp.savetxt('multilingual_review_xlnet_accuracy_tracker.txt',accuracy_tracker,delimiter=',')\nnp.savetxt('multilingual_review_xlnet_f1_micro_tracker.txt',f1_micro_tracker,delimiter=',')\nnp.savetxt('multilingual_review_xlnet_f1_macro_tracker.txt',f1_macro_tracker,delimiter=',')\n\nPkl_Filename = \"multingual_review_xlnet.pkl\"  \n\nwith open(Pkl_Filename, 'wb') as file:  \n    pickle.dump(model, file)'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# ... (your existing code)\n\naccuracy_tracker = []\nf1_micro_tracker = []\nf1_macro_tracker = []\nprecision_tracker = []\nrecall_tracker = []\nlog_loss_tracker = []\nhamming_loss_tracker = []\n\nfor epoch in range(EPOCHS):\n    print(\"Epoch \",epoch)\n    train(epoch)\n    outputs, targets = validation(epoch)\n    outputs = np.array(outputs) >= 0.5\n\n    accuracy = metrics.accuracy_score(targets, outputs)\n    f1_score_micro = metrics.f1_score(targets, outputs, average='micro')\n    f1_score_macro = metrics.f1_score(targets, outputs, average='macro')\n    \n    # Calculate precision, recall\n    precision = metrics.precision_score(targets, outputs, average='micro')\n    recall = metrics.recall_score(targets, outputs, average='micro')\n\n    # Calculate log loss\n    log_loss_val = metrics.log_loss(targets, outputs)\n\n    # Calculate hamming loss\n    hamming_loss_val = metrics.hamming_loss(targets, outputs)\n\n    accuracy_tracker.append(accuracy)\n    f1_micro_tracker.append(f1_score_micro)\n    f1_macro_tracker.append(f1_score_macro)\n    precision_tracker.append(precision)\n    recall_tracker.append(recall)\n    log_loss_tracker.append(log_loss_val)\n    hamming_loss_tracker.append(hamming_loss_val)\n\n    print(f\"Accuracy Score = {accuracy}\")\n    print(f\"F1 Score (Micro) = {f1_score_micro}\")\n    print(f\"F1 Score (Macro) = {f1_score_macro}\")\n    print(f\"Precision = {precision}\")\n    print(f\"Recall = {recall}\")\n    print(f\"Log Loss = {log_loss_val}\")\n    print(f\"Hamming Loss = {hamming_loss_val}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-05T02:16:56.703108Z","iopub.execute_input":"2023-12-05T02:16:56.703530Z","iopub.status.idle":"2023-12-05T03:53:15.101145Z","shell.execute_reply.started":"2023-12-05T02:16:56.703496Z","shell.execute_reply":"2023-12-05T03:53:15.099985Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"Epoch  0\nEpoch: 0, Loss:  0.8704323768615723\nEpoch: 0, Loss:  0.40374475717544556\nEpoch: 0, Loss:  0.5173176527023315\nEpoch: 0, Loss:  0.3137751519680023\nEpoch: 0, Loss:  0.6635217666625977\nEpoch: 0, Loss:  0.2858777642250061\nAccuracy Score = 0.5003\nF1 Score (Micro) = 0.5004501350405122\nF1 Score (Macro) = 0.3342883531733652\nPrecision = 0.5006003602161296\nRecall = 0.5003\nLog Loss = 17.989803294816703\nHamming Loss = 0.4994\nEpoch  1\nEpoch: 1, Loss:  0.25238949060440063\nEpoch: 1, Loss:  0.5585943460464478\nEpoch: 1, Loss:  0.30866703391075134\nEpoch: 1, Loss:  0.15207642316818237\nEpoch: 1, Loss:  0.8928340673446655\nEpoch: 1, Loss:  0.6097288131713867\nAccuracy Score = 0.574\nF1 Score (Micro) = 0.5764305722288916\nF1 Score (Macro) = 0.5191473515583979\nPrecision = 0.5766613290632506\nRecall = 0.5762\nLog Loss = 15.170773711479406\nHamming Loss = 0.4234\nEpoch  2\nEpoch: 2, Loss:  0.6176093816757202\nEpoch: 2, Loss:  0.2725371718406677\nEpoch: 2, Loss:  0.4001775085926056\nEpoch: 2, Loss:  0.2752133309841156\nEpoch: 2, Loss:  0.4426409602165222\nEpoch: 2, Loss:  0.4788898527622223\nAccuracy Score = 0.566\nF1 Score (Micro) = 0.5680473372781065\nF1 Score (Macro) = 0.48412558716432164\nPrecision = 0.5697042848521424\nRecall = 0.5664\nLog Loss = 15.409632229900366\nHamming Loss = 0.4307\n","output_type":"stream"}]},{"cell_type":"code","source":"# Calculate final scores after all epochs\nfinal_accuracy = np.mean(accuracy_tracker)\nfinal_f1_micro = np.mean(f1_micro_tracker)\nfinal_f1_macro = np.mean(f1_macro_tracker)\nfinal_precision = np.mean(precision_tracker)\nfinal_recall = np.mean(recall_tracker)\nfinal_log_loss = np.mean(log_loss_tracker)\nfinal_hamming_loss = np.mean(hamming_loss_tracker)\n\n# Print final scores\nprint(\"Final Accuracy Score = {:.4f}\".format(final_accuracy))\nprint(\"Final F1 Score (Micro) = {:.4f}\".format(final_f1_micro))\nprint(\"Final F1 Score (Macro) = {:.4f}\".format(final_f1_macro))\nprint(\"Final Precision = {:.4f}\".format(final_precision))\nprint(\"Final Recall = {:.4f}\".format(final_recall))\nprint(\"Final Log Loss = {:.4f}\".format(final_log_loss))\nprint(\"Final Hamming Loss = {:.4f}\".format(final_hamming_loss))\n\n# Save metrics to files\nnp.savetxt('multilingual_review_xlnet_precision_tracker.txt', precision_tracker, delimiter=',')\nnp.savetxt('multilingual_review_xlnet_recall_tracker.txt', recall_tracker, delimiter=',')\nnp.savetxt('multilingual_review_xlnet_log_loss_tracker.txt', log_loss_tracker, delimiter=',')\nnp.savetxt('multilingual_review_xlnet_hamming_loss_tracker.txt', hamming_loss_tracker, delimiter=',')\n","metadata":{"execution":{"iopub.status.busy":"2023-12-05T03:53:15.133268Z","iopub.execute_input":"2023-12-05T03:53:15.134063Z","iopub.status.idle":"2023-12-05T03:53:15.146634Z","shell.execute_reply.started":"2023-12-05T03:53:15.134036Z","shell.execute_reply":"2023-12-05T03:53:15.145674Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Final Accuracy Score = 0.5468\nFinal F1 Score (Micro) = 0.5483\nFinal F1 Score (Macro) = 0.4459\nFinal Precision = 0.5490\nFinal Recall = 0.5476\nFinal Log Loss = 16.1901\nFinal Hamming Loss = 0.4512\n","output_type":"stream"}]}]}