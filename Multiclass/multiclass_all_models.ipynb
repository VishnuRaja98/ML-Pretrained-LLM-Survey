{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1a737754-6017-414c-a5d7-79023461f809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==2.0.0+cu117 in ./anaconda3/lib/python3.9/site-packages (2.0.0+cu117)\n",
      "Requirement already satisfied: sympy in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.0+cu117) (1.10.1)\n",
      "Requirement already satisfied: jinja2 in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.0+cu117) (2.11.3)\n",
      "Requirement already satisfied: filelock in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.0+cu117) (3.6.0)\n",
      "Requirement already satisfied: typing-extensions in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.0+cu117) (4.8.0)\n",
      "Requirement already satisfied: networkx in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.0+cu117) (2.7.1)\n",
      "Requirement already satisfied: triton==2.0.0 in ./anaconda3/lib/python3.9/site-packages (from torch==2.0.0+cu117) (2.0.0)\n",
      "Requirement already satisfied: lit in ./anaconda3/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.0+cu117) (17.0.6)\n",
      "Requirement already satisfied: cmake in ./anaconda3/lib/python3.9/site-packages (from triton==2.0.0->torch==2.0.0+cu117) (3.27.9)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in ./anaconda3/lib/python3.9/site-packages (from jinja2->torch==2.0.0+cu117) (2.0.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./anaconda3/lib/python3.9/site-packages (from sympy->torch==2.0.0+cu117) (1.2.1)\n",
      "Requirement already satisfied: tokenizers in ./anaconda3/lib/python3.9/site-packages (0.15.0)\n",
      "Requirement already satisfied: huggingface_hub<1.0,>=0.16.4 in ./anaconda3/lib/python3.9/site-packages (from tokenizers) (0.19.4)\n",
      "Requirement already satisfied: packaging>=20.9 in ./anaconda3/lib/python3.9/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (21.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./anaconda3/lib/python3.9/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2023.10.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./anaconda3/lib/python3.9/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (6.0)\n",
      "Requirement already satisfied: filelock in ./anaconda3/lib/python3.9/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (3.6.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in ./anaconda3/lib/python3.9/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.64.0)\n",
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.9/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./anaconda3/lib/python3.9/site-packages (from huggingface_hub<1.0,>=0.16.4->tokenizers) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./anaconda3/lib/python3.9/site-packages (from packaging>=20.9->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./anaconda3/lib/python3.9/site-packages (from requests->huggingface_hub<1.0,>=0.16.4->tokenizers) (2.0.4)\n",
      "Requirement already satisfied: transformers in ./anaconda3/lib/python3.9/site-packages (4.35.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./anaconda3/lib/python3.9/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./anaconda3/lib/python3.9/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./anaconda3/lib/python3.9/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in ./anaconda3/lib/python3.9/site-packages (from transformers) (0.19.4)\n",
      "Requirement already satisfied: filelock in ./anaconda3/lib/python3.9/site-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./anaconda3/lib/python3.9/site-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in ./anaconda3/lib/python3.9/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: requests in ./anaconda3/lib/python3.9/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in ./anaconda3/lib/python3.9/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./anaconda3/lib/python3.9/site-packages (from transformers) (2022.3.15)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./anaconda3/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.8.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in ./anaconda3/lib/python3.9/site-packages (from packaging>=20.0->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers) (1.26.18)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in ./anaconda3/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==2.0.0+cu117 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "!pip install tokenizers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "07190bea-01bd-4747-8738-5f64ea5ed92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('2.0.0+cu117', '4.35.2', '0.15.0')"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch, transformers, tokenizers\n",
    "torch.__version__, transformers.__version__, tokenizers.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "654cf227-8d91-4e37-8847-61abe8f7f289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Let's use 3 GPUs!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, json, gc, re, random\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from torch.nn import DataParallel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification\n",
    "from transformers import XLNetTokenizer, XLNetForSequenceClassification\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from sklearn.metrics import precision_score, recall_score, matthews_corrcoef, log_loss, hamming_loss\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# %matplotlib inline\n",
    "# import plotly.express as px\n",
    "# import seaborn as sns\n",
    "print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6a8b9b3b-f30d-4092-be2d-d022d4fea34b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Release Year</th>\n",
       "      <th>Title</th>\n",
       "      <th>Origin/Ethnicity</th>\n",
       "      <th>Director</th>\n",
       "      <th>Cast</th>\n",
       "      <th>Genre</th>\n",
       "      <th>Wiki Page</th>\n",
       "      <th>Plot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1901</td>\n",
       "      <td>Kansas Saloon Smashers</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...</td>\n",
       "      <td>A bartender is working at a saloon, serving dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1901</td>\n",
       "      <td>Love by the Light of the Moon</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Love_by_the_Ligh...</td>\n",
       "      <td>The moon, painted with a smiling face hangs ov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1901</td>\n",
       "      <td>The Martyred Presidents</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Martyred_Pre...</td>\n",
       "      <td>The film, just over a minute long, is composed...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1901</td>\n",
       "      <td>Terrible Teddy, the Grizzly King</td>\n",
       "      <td>American</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Terrible_Teddy,_...</td>\n",
       "      <td>Lasting just 61 seconds and consisting of two ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1902</td>\n",
       "      <td>Jack and the Beanstalk</td>\n",
       "      <td>American</td>\n",
       "      <td>George S. Fleming, Edwin S. Porter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Jack_and_the_Bea...</td>\n",
       "      <td>The earliest known adaptation of the classic f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34881</th>\n",
       "      <td>2014</td>\n",
       "      <td>The Water Diviner</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Director: Russell Crowe</td>\n",
       "      <td>Director: Russell Crowe\\r\\nCast: Russell Crowe...</td>\n",
       "      <td>unknown</td>\n",
       "      <td>https://en.wikipedia.org/wiki/The_Water_Diviner</td>\n",
       "      <td>The film begins in 1919, just after World War ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34882</th>\n",
       "      <td>2017</td>\n",
       "      <td>Çalgı Çengi İkimiz</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Selçuk Aydemir</td>\n",
       "      <td>Ahmet Kural, Murat Cemcir</td>\n",
       "      <td>comedy</td>\n",
       "      <td>https://en.wikipedia.org/wiki/%C3%87alg%C4%B1_...</td>\n",
       "      <td>Two musicians, Salih and Gürkan, described the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34883</th>\n",
       "      <td>2017</td>\n",
       "      <td>Olanlar Oldu</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Hakan Algül</td>\n",
       "      <td>Ata Demirer, Tuvana Türkay, Ülkü Duru</td>\n",
       "      <td>comedy</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Olanlar_Oldu</td>\n",
       "      <td>Zafer, a sailor living with his mother Döndü i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34884</th>\n",
       "      <td>2017</td>\n",
       "      <td>Non-Transferable</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Brendan Bradley</td>\n",
       "      <td>YouTubers Shanna Malcolm, Shira Lazar, Sara Fl...</td>\n",
       "      <td>romantic comedy</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Non-Transferable...</td>\n",
       "      <td>The film centres around a young woman named Am...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34885</th>\n",
       "      <td>2017</td>\n",
       "      <td>İstanbul Kırmızısı</td>\n",
       "      <td>Turkish</td>\n",
       "      <td>Ferzan Özpetek</td>\n",
       "      <td>Halit Ergenç, Tuba Büyüküstün, Mehmet Günsür, ...</td>\n",
       "      <td>romantic</td>\n",
       "      <td>https://en.wikipedia.org/wiki/%C4%B0stanbul_K%...</td>\n",
       "      <td>The writer Orhan Şahin returns to İstanbul aft...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34886 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Release Year                             Title Origin/Ethnicity  \\\n",
       "0              1901            Kansas Saloon Smashers         American   \n",
       "1              1901     Love by the Light of the Moon         American   \n",
       "2              1901           The Martyred Presidents         American   \n",
       "3              1901  Terrible Teddy, the Grizzly King         American   \n",
       "4              1902            Jack and the Beanstalk         American   \n",
       "...             ...                               ...              ...   \n",
       "34881          2014                 The Water Diviner          Turkish   \n",
       "34882          2017                Çalgı Çengi İkimiz          Turkish   \n",
       "34883          2017                      Olanlar Oldu          Turkish   \n",
       "34884          2017                  Non-Transferable          Turkish   \n",
       "34885          2017                İstanbul Kırmızısı          Turkish   \n",
       "\n",
       "                                 Director  \\\n",
       "0                                 Unknown   \n",
       "1                                 Unknown   \n",
       "2                                 Unknown   \n",
       "3                                 Unknown   \n",
       "4      George S. Fleming, Edwin S. Porter   \n",
       "...                                   ...   \n",
       "34881             Director: Russell Crowe   \n",
       "34882                      Selçuk Aydemir   \n",
       "34883                         Hakan Algül   \n",
       "34884                     Brendan Bradley   \n",
       "34885                      Ferzan Özpetek   \n",
       "\n",
       "                                                    Cast            Genre  \\\n",
       "0                                                    NaN          unknown   \n",
       "1                                                    NaN          unknown   \n",
       "2                                                    NaN          unknown   \n",
       "3                                                    NaN          unknown   \n",
       "4                                                    NaN          unknown   \n",
       "...                                                  ...              ...   \n",
       "34881  Director: Russell Crowe\\r\\nCast: Russell Crowe...          unknown   \n",
       "34882                          Ahmet Kural, Murat Cemcir           comedy   \n",
       "34883              Ata Demirer, Tuvana Türkay, Ülkü Duru           comedy   \n",
       "34884  YouTubers Shanna Malcolm, Shira Lazar, Sara Fl...  romantic comedy   \n",
       "34885  Halit Ergenç, Tuba Büyüküstün, Mehmet Günsür, ...         romantic   \n",
       "\n",
       "                                               Wiki Page  \\\n",
       "0      https://en.wikipedia.org/wiki/Kansas_Saloon_Sm...   \n",
       "1      https://en.wikipedia.org/wiki/Love_by_the_Ligh...   \n",
       "2      https://en.wikipedia.org/wiki/The_Martyred_Pre...   \n",
       "3      https://en.wikipedia.org/wiki/Terrible_Teddy,_...   \n",
       "4      https://en.wikipedia.org/wiki/Jack_and_the_Bea...   \n",
       "...                                                  ...   \n",
       "34881    https://en.wikipedia.org/wiki/The_Water_Diviner   \n",
       "34882  https://en.wikipedia.org/wiki/%C3%87alg%C4%B1_...   \n",
       "34883         https://en.wikipedia.org/wiki/Olanlar_Oldu   \n",
       "34884  https://en.wikipedia.org/wiki/Non-Transferable...   \n",
       "34885  https://en.wikipedia.org/wiki/%C4%B0stanbul_K%...   \n",
       "\n",
       "                                                    Plot  \n",
       "0      A bartender is working at a saloon, serving dr...  \n",
       "1      The moon, painted with a smiling face hangs ov...  \n",
       "2      The film, just over a minute long, is composed...  \n",
       "3      Lasting just 61 seconds and consisting of two ...  \n",
       "4      The earliest known adaptation of the classic f...  \n",
       "...                                                  ...  \n",
       "34881  The film begins in 1919, just after World War ...  \n",
       "34882  Two musicians, Salih and Gürkan, described the...  \n",
       "34883  Zafer, a sailor living with his mother Döndü i...  \n",
       "34884  The film centres around a young woman named Am...  \n",
       "34885  The writer Orhan Şahin returns to İstanbul aft...  \n",
       "\n",
       "[34886 rows x 8 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df = pd.read_csv(\"wiki_movie_plots_deduped.csv\")\n",
    "movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e57dc1ab-cb46-4a4d-99c2-0e71df32e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_df = movies_df[(movies_df[\"Origin/Ethnicity\"]==\"American\") | (movies_df[\"Origin/Ethnicity\"]==\"British\")]\n",
    "movies_df = movies_df[[\"Plot\", \"Genre\"]]\n",
    "drop_indices = movies_df[movies_df[\"Genre\"] == \"unknown\" ].index\n",
    "movies_df.drop(drop_indices, inplace=True)\n",
    "\n",
    "# Combine genres: 1) \"sci-fi\" with \"science fiction\" &  2) \"romantic comedy\" with \"romance\"\n",
    "movies_df[\"Genre\"].replace({\"sci-fi\": \"science fiction\", \"romantic comedy\": \"romance\"}, inplace=True)\n",
    "\n",
    "# Choosing movie genres based on their frequency\n",
    "shortlisted_genres = movies_df[\"Genre\"].value_counts().reset_index(name=\"count\").query(\"count > 200\")[\"index\"].tolist()\n",
    "movies_df = movies_df[movies_df[\"Genre\"].isin(shortlisted_genres)].reset_index(drop=True)\n",
    "\n",
    "# Shuffle DataFrame\n",
    "movies_df = movies_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# Sample roughly equal number of movie plots from different genres (to reduce class imbalance issues)\n",
    "movies_df = movies_df.groupby(\"Genre\").head(400).reset_index(drop=True)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "movies_df[\"genre_encoded\"] = label_encoder.fit_transform(movies_df[\"Genre\"].tolist())\n",
    "\n",
    "movies_df = movies_df[[\"Plot\", \"Genre\", \"genre_encoded\"]]\n",
    "movies_df\n",
    "\n",
    "def tokenize_data(dataframe, tokenizer):\n",
    "    return tokenizer(dataframe[\"Plot\"].tolist(), padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "# Label encoding for genres\n",
    "label_encoder = LabelEncoder()\n",
    "movies_df[\"genre_encoded\"] = label_encoder.fit_transform(movies_df[\"Genre\"])\n",
    "\n",
    "\n",
    "train_df, eval_df = train_test_split(movies_df, test_size=0.2, stratify=movies_df[\"Genre\"], random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "dcb24c64-1081-4207-8b34-353273a317fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import DataParallel\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import precision_score, recall_score, matthews_corrcoef, log_loss, hamming_loss\n",
    "from torch.nn.functional import softmax\n",
    "import numpy as np\n",
    "\n",
    "# Define your custom train_and_evaluate function\n",
    "def train_and_evaluate(model, train_loader, eval_loader, device, optimizer=None, criterion=None):\n",
    "    if optimizer is None:\n",
    "        optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        print(f\"Let's use {torch.cuda.device_count()} GPUs!\")\n",
    "        model = DataParallel(model)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Training Loop\n",
    "    for epoch in range(2):  # Adjust the number of epochs as needed\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = criterion(outputs.logits, batch['labels'])\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        average_train_loss = total_train_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} completed, Average Training Loss: {average_train_loss:.4f}\")\n",
    "\n",
    "    # Evaluation Loop\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    all_probs = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in eval_loader:\n",
    "            inputs = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "            logits = outputs.logits\n",
    "            probs = softmax(logits, dim=1)\n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "\n",
    "            predictions = torch.argmax(logits, dim=-1)\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_true_labels.extend(inputs['labels'].cpu().numpy())\n",
    "\n",
    "    # Calculate Metrics\n",
    "    precision = precision_score(all_true_labels, all_predictions, average='weighted', zero_division=0)\n",
    "    recall = recall_score(all_true_labels, all_predictions, average='weighted')\n",
    "    mcc = matthews_corrcoef(all_true_labels, all_predictions)\n",
    "    logloss = log_loss(all_true_labels, all_probs, labels=np.unique(all_true_labels))\n",
    "    hammingloss = hamming_loss(all_true_labels, all_predictions)\n",
    "\n",
    "    print(f\"Precision: {precision:.4f}, Recall: {recall:.4f}, MCC: {mcc:.4f}, Log Loss: {logloss:.4f}, Hamming Loss: {hammingloss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "603c8c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "class MovieDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "e1c92788",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# BERT\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "bert_train_encodings = tokenize_data(train_df, bert_tokenizer)\n",
    "bert_eval_encodings = tokenize_data(eval_df, bert_tokenizer)\n",
    "\n",
    "bert_train_dataset = MovieDataset(bert_train_encodings, train_df[\"genre_encoded\"].tolist())\n",
    "bert_eval_dataset = MovieDataset(bert_eval_encodings, eval_df[\"genre_encoded\"].tolist())\n",
    "\n",
    "bert_train_loader = DataLoader(bert_train_dataset, batch_size=18, shuffle=True)\n",
    "bert_eval_loader = DataLoader(bert_eval_dataset, batch_size=18)\n",
    "\n",
    "bert_model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "bert_optimizer = AdamW(bert_model.parameters(), lr=5e-5)\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "6bf4ada1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# RoBERTa Tokenizer and Model\n",
    "roberta_tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "roberta_train_encodings = tokenize_data(train_df, roberta_tokenizer)\n",
    "roberta_eval_encodings = tokenize_data(eval_df, roberta_tokenizer)\n",
    "\n",
    "roberta_train_dataset = MovieDataset(roberta_train_encodings, train_df[\"genre_encoded\"].tolist())\n",
    "roberta_eval_dataset = MovieDataset(roberta_eval_encodings, eval_df[\"genre_encoded\"].tolist())\n",
    "\n",
    "roberta_train_loader = DataLoader(roberta_train_dataset, batch_size=18, shuffle=True)\n",
    "roberta_eval_loader = DataLoader(roberta_eval_dataset, batch_size=18)\n",
    "\n",
    "roberta_model = RobertaForSequenceClassification.from_pretrained('roberta-base', num_labels=len(label_encoder.classes_))\n",
    "roberta_optimizer = AdamW(roberta_model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "765e96cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.weight', 'sequence_summary.summary.bias', 'logits_proj.bias', 'sequence_summary.summary.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# XLNet Tokenizer and Model\n",
    "xlnet_tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n",
    "xlnet_train_encodings = tokenize_data(train_df, xlnet_tokenizer)\n",
    "xlnet_eval_encodings = tokenize_data(eval_df, xlnet_tokenizer)\n",
    "\n",
    "xlnet_train_dataset = MovieDataset(xlnet_train_encodings, train_df[\"genre_encoded\"].tolist())\n",
    "xlnet_eval_dataset = MovieDataset(xlnet_eval_encodings, eval_df[\"genre_encoded\"].tolist())\n",
    "\n",
    "xlnet_train_loader = DataLoader(xlnet_train_dataset, batch_size=18, shuffle=True)\n",
    "xlnet_eval_loader = DataLoader(xlnet_eval_dataset, batch_size=18)\n",
    "\n",
    "xlnet_model = XLNetForSequenceClassification.from_pretrained('xlnet-base-cased', num_labels=len(label_encoder.classes_))\n",
    "xlnet_optimizer = AdamW(xlnet_model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d658f98c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# DistilBERT Tokenizer and Model\n",
    "distilbert_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "distilbert_train_encodings = tokenize_data(train_df, distilbert_tokenizer)\n",
    "distilbert_eval_encodings = tokenize_data(eval_df, distilbert_tokenizer)\n",
    "\n",
    "distilbert_train_dataset = MovieDataset(distilbert_train_encodings, train_df[\"genre_encoded\"].tolist())\n",
    "distilbert_eval_dataset = MovieDataset(distilbert_eval_encodings, eval_df[\"genre_encoded\"].tolist())\n",
    "\n",
    "distilbert_train_loader = DataLoader(distilbert_train_dataset, batch_size=18, shuffle=True)\n",
    "distilbert_eval_loader = DataLoader(distilbert_eval_dataset, batch_size=18)\n",
    "\n",
    "distilbert_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=len(label_encoder.classes_))\n",
    "distilbert_optimizer = AdamW(distilbert_model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "866af5f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating BERT...\n",
      "Let's use 3 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed, Average Training Loss: 1.9047\n",
      "Epoch 2 completed, Average Training Loss: 1.2722\n",
      "Precision: 0.5497, Recall: 0.5539, MCC: 0.5253, Log Loss: 1.3679, Hamming Loss: 0.4461\n",
      "Training and evaluating RoBERTa...\n",
      "Let's use 3 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed, Average Training Loss: 1.9201\n",
      "Epoch 2 completed, Average Training Loss: 1.3905\n",
      "Precision: 0.5425, Recall: 0.5379, MCC: 0.5075, Log Loss: 1.4215, Hamming Loss: 0.4621\n",
      "Training and evaluating XLNet...\n",
      "Let's use 3 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed, Average Training Loss: 1.9206\n",
      "Epoch 2 completed, Average Training Loss: 1.3391\n",
      "Precision: 0.5510, Recall: 0.5017, MCC: 0.4721, Log Loss: 1.4813, Hamming Loss: 0.4983\n",
      "Training and evaluating DistilBERT...\n",
      "Let's use 3 GPUs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/brian/anaconda3/lib/python3.9/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed, Average Training Loss: 1.8812\n",
      "Epoch 2 completed, Average Training Loss: 1.2547\n",
      "Precision: 0.5685, Recall: 0.5513, MCC: 0.5227, Log Loss: 1.3659, Hamming Loss: 0.4487\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "criterion = nn.CrossEntropyLoss(reduction='mean')\n",
    "# Train and Evaluate BERT\n",
    "print(\"Training and evaluating BERT...\")\n",
    "train_and_evaluate(bert_model, bert_train_loader, bert_eval_loader, device,optimizer=bert_optimizer,criterion=criterion)\n",
    "\n",
    "# Train and Evaluate RoBERTa\n",
    "print(\"Training and evaluating RoBERTa...\")\n",
    "train_and_evaluate(roberta_model, roberta_train_loader, roberta_eval_loader, device,optimizer=roberta_optimizer,criterion=criterion)\n",
    "\n",
    "\n",
    "# Train and Evaluate XLNet\n",
    "print(\"Training and evaluating XLNet...\")\n",
    "train_and_evaluate(xlnet_model, xlnet_train_loader, xlnet_eval_loader, device,optimizer=xlnet_optimizer,criterion=criterion)\n",
    "\n",
    "\n",
    "# Train and Evaluate DistilBERT\n",
    "print(\"Training and evaluating DistilBERT...\")\n",
    "train_and_evaluate(distilbert_model, distilbert_train_loader, distilbert_eval_loader, device,optimizer=distilbert_optimizer,criterion=criterion)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2554a26f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
